{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter data\n",
    "Many social networks, including Facebook, twitter, OkCupid have information about geolocation. \n",
    "Using geolocation data one can get or infer the information about some things happening around the world. This notebook has been inspired from the work about surfers locations. \n",
    "\n",
    "As the idea we can use \n",
    "1. Openhumans.org https://exploratory.openhumans.org/notebook/1/\n",
    "2. Public repositories https://gwu-libraries.github.io/sfm-ui/posts/2017-09-14-twitter-data networkrepository.org \n",
    "3. DOLLY (Digital Online Life and You) data http://www.floatingsheep.org/2010/01/googles-geographies-of-religion.html  \n",
    "\n",
    "\n",
    "<img src=\"Where_Surfers_Travel.png\" alt=\"Drawing\" style=\"width: 900px;\"/>\n",
    "\n",
    "\n",
    "*Notebook dedicated to the visit of Lisa from CorrelAid*\n",
    "\n",
    "### Main questions of this notebook \n",
    "\n",
    "1. Here using some open twitter data we infer the information about the main emergency events in the world. \n",
    "2. We also try to infer the information about human travels. \n",
    "3. Using twitter data we can also show that normal borders, which exist on our maps do not really exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "import zipfile\n",
    "import pytz\n",
    "import io\n",
    "import sys\n",
    "from textblob import TextBlob\n",
    "#import emoji\n",
    "\n",
    "# time for twitter data\n",
    "DARKSKY_KEY = ''\n",
    "DATARANGE_START = \"2016-06-01\"\n",
    "DATARANGE_END = \"2018-05-08\"\n",
    "\n",
    "\n",
    "\n",
    "# sets the axis label sizes for seaborn as in Bastian setup\n",
    "rc={'font.size': 14, 'axes.labelsize': 14, 'legend.fontsize': 14.0, \n",
    "    'axes.titlesize': 14, 'xtick.labelsize': 14, 'ytick.labelsize': 14}\n",
    "sns.set(rc=rc)\n",
    "\n",
    "\n",
    "# THIS CODE BELOW IS COPIED FROM TWARXIV.ORG AS IT ALREADY DOES EXACTLY WHAT WE WANT FOR READING IN THE DATA\n",
    "# This code is also using some Bastian code  https://exploratory.openhumans.org/notebook/1/ \n",
    "# Other authors: Liuba, Lisa, Jon\n",
    "\n",
    "# READ JSON FILES FROM TWITTER ARCHIVE!\n",
    "\n",
    "def check_hashtag(single_tweet):\n",
    "    '''check whether tweet has any hashtags'''\n",
    "    return len(single_tweet['entities']['hashtags']) > 0\n",
    "\n",
    "\n",
    "def check_media(single_tweet):\n",
    "    '''check whether tweet has any media attached'''\n",
    "    return len(single_tweet['entities']['media']) > 0\n",
    "\n",
    "\n",
    "def check_url(single_tweet):\n",
    "    '''check whether tweet has any urls attached'''\n",
    "    return len(single_tweet['entities']['urls']) > 0\n",
    "\n",
    "\n",
    "def check_retweet(single_tweet):\n",
    "    '''\n",
    "    check whether tweet is a RT. If yes:\n",
    "    return name & user name of the RT'd user.\n",
    "    otherwise just return nones\n",
    "    '''\n",
    "    if 'retweeted_status' in single_tweet.keys():\n",
    "        return (single_tweet['retweeted_status']['user']['screen_name'],\n",
    "                single_tweet['retweeted_status']['user']['name'])\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "def check_coordinates(single_tweet):\n",
    "    '''\n",
    "    check whether tweet has coordinates of location attached.\n",
    "    if yes return the coordinates\n",
    "    otherwise just return nones\n",
    "    '''\n",
    "    if 'coordinates' in single_tweet['geo'].keys():\n",
    "        return (single_tweet['geo']['coordinates'][0],\n",
    "                single_tweet['geo']['coordinates'][1])\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "def check_reply_to(single_tweet):\n",
    "    '''\n",
    "    check whether tweet is a reply. If yes:\n",
    "    return name & user name of the user that's replied to.\n",
    "    otherwise just return nones\n",
    "    '''\n",
    "    if 'in_reply_to_screen_name' in single_tweet.keys():\n",
    "        name = None\n",
    "        for user in single_tweet['entities']['user_mentions']:\n",
    "            if user['screen_name'] == single_tweet['in_reply_to_screen_name']:\n",
    "                name = user['name']\n",
    "                break\n",
    "        return (single_tweet['in_reply_to_screen_name'], name)\n",
    "    else:\n",
    "        return (None, None)\n",
    "\n",
    "\n",
    "def create_dataframe(tweets):\n",
    "    '''\n",
    "    create a pandas dataframe from our tweet jsons\n",
    "    '''\n",
    "\n",
    "    # initalize empty lists\n",
    "    utc_time = []\n",
    "    longitude = []\n",
    "    latitude = []\n",
    "    hashtag = []\n",
    "    media = []\n",
    "    url = []\n",
    "    retweet_user_name = []\n",
    "    retweet_name = []\n",
    "    reply_user_name = []\n",
    "    reply_name = []\n",
    "    text = []\n",
    "    # iterate over all tweets and extract data\n",
    "    for single_tweet in tweets:\n",
    "        utc_time.append(datetime.strptime(single_tweet['created_at'],\n",
    "                                                   '%Y-%m-%d %H:%M:%S %z'))\n",
    "        coordinates = check_coordinates(single_tweet)\n",
    "        latitude.append(coordinates[0])\n",
    "        longitude.append(coordinates[1])\n",
    "        hashtag.append(check_hashtag(single_tweet))\n",
    "        media.append(check_media(single_tweet))\n",
    "        url.append(check_url(single_tweet))\n",
    "        retweet = check_retweet(single_tweet)\n",
    "        retweet_user_name.append(retweet[0])\n",
    "        retweet_name.append(retweet[1])\n",
    "        reply = check_reply_to(single_tweet)\n",
    "        reply_user_name.append(reply[0])\n",
    "        reply_name.append(reply[1])\n",
    "        text.append(single_tweet['text'])\n",
    "    # convert the whole shebang into a pandas dataframe\n",
    "    dataframe = pd.DataFrame(data={\n",
    "                            'utc_time': utc_time,\n",
    "                            'latitude': latitude,\n",
    "                            'longitude': longitude,\n",
    "                            'hashtag': hashtag,\n",
    "                            'media': media,\n",
    "                            'url': url,\n",
    "                            'retweet_user_name': retweet_user_name,\n",
    "                            'retweet_name': retweet_name,\n",
    "                            'reply_user_name': reply_user_name,\n",
    "                            'reply_name': reply_name,\n",
    "                            'text': text\n",
    "    })\n",
    "    return dataframe\n",
    "\n",
    "\n",
    "def read_files(zip_url):\n",
    "    tf = tempfile.NamedTemporaryFile()\n",
    "    print('downloading files')\n",
    "    tf.write(requests.get(zip_url).content)\n",
    "    tf.flush()\n",
    "    zf = zipfile.ZipFile(tf.name)\n",
    "    print('reading index')\n",
    "    with zf.open('data/js/tweet_index.js', 'r') as f:\n",
    "        f = io.TextIOWrapper(f)\n",
    "        d = f.readlines()[1:]\n",
    "        d = \"[{\" + \"\".join(d)\n",
    "        json_files = json.loads(d)\n",
    "    data_frames = []\n",
    "    print('iterate over individual files')\n",
    "    for single_file in json_files:\n",
    "        print('read ' + single_file['file_name'])\n",
    "        with zf.open(single_file['file_name']) as f:\n",
    "            f = io.TextIOWrapper(f)\n",
    "            d = f.readlines()[1:]\n",
    "            d = \"\".join(d)\n",
    "            tweets = json.loads(d)\n",
    "            df_tweets = create_dataframe(tweets)\n",
    "            data_frames.append(df_tweets)\n",
    "    return data_frames\n",
    "\n",
    "\n",
    "def create_main_dataframe(zip_url='http://ruleofthirds.de/test_archive.zip'):\n",
    "    print('reading files')\n",
    "    dataframes = read_files(zip_url)\n",
    "    print('concatenating...')\n",
    "    dataframe = pd.concat(dataframes)\n",
    "    dataframe = dataframe.sort_values('utc_time', ascending=False)\n",
    "    dataframe = dataframe.set_index('utc_time')\n",
    "    dataframe = dataframe.replace(to_replace={\n",
    "                                    'url': {False: None},\n",
    "                                    'hashtag': {False: None},\n",
    "                                    'media': {False: None}\n",
    "                                    })\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data and analyzing locations\n",
    "\n",
    "Let us plot the coordinates of twitter user using function *check_coordinates(single_tweet)* \n",
    "For each user we will have its own trajectory of where he/she tweeted.\n",
    "\n",
    "The goal is later to get geographical locations of tweets with #tags which mention conferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"detail\": \"Invalid token.\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# get our download URLs\\nfor entry in user[\\'data\\']:\\n    if entry[\\'source\\'] == \"direct-sharing-70\":\\n        twitter_data_url = entry[\\'download_url\\']\\n        has_twitter = True\\n    if entry[\\'source\\'] == \"direct-sharing-138\":\\n        moves_data_url = entry[\\'download_url\\']\\n        has_moves = True\\nif not has_twitter:\\n    print(\"YOU NEED TO HAVE SOME TWITTER DATA IN YOUR ACCOUNT TO USE THIS NOTEBOOK\")\\n    print(\"GO TO http://twarxiv.org TO UPLOAD IT\")\\n\\n\\n\\n# read the twitter data\\n#twitter_data = create_main_dataframe(zip_url=twitter_data_url)\\n\\n# load json file: convert js file to json and then read it\\ntwitter_file = \\'C:/Users/lyubo/Documents/PYTHON/jupiter_notebook/twitter_data_analysis/account1/data/js/tweet_index.js\\'\\n\\n\\n\\nwith open(twitter_file) as dataFile:\\n    data = dataFile.read()\\n    obj = data[data.find(\\'{\\') : data.rfind(\\'}\\')+1]\\n    jsonObj = json.loads(obj)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import os \n",
    "'''\n",
    "\n",
    "# read data from online file of OpenHumans\n",
    "response = requests.get(\"https://www.openhumans.org/api/direct-sharing/project/exchange-member/?access_token={}\".format(os.environ.get('OH_ACCESS_TOKEN')))\n",
    "user = json.loads(response.content)\n",
    "has_twitter = False\n",
    "\n",
    "public_data = {}\n",
    "for i in response['results']:\n",
    "    data = requests.get(i['download_url']).json()\n",
    "    public_data[i['user']['username']] = data\n",
    "        \n",
    "''' \n",
    "\n",
    "response = requests.get(\"https://www.openhumans.org/api/direct-sharing/project/exchange-member/?access_token={}\".format(os.environ.get('OH_ACCESS_TOKEN')))\n",
    "user = json.loads(response.content)\n",
    "\n",
    "\n",
    "#printing data \n",
    "print(json.dumps(user, indent=4, sort_keys=True))\n",
    "\n",
    "has_twitter = False\n",
    "has_moves = False\n",
    "'''\n",
    "# get our download URLs\n",
    "for entry in user['data']:\n",
    "    if entry['source'] == \"direct-sharing-70\":\n",
    "        twitter_data_url = entry['download_url']\n",
    "        has_twitter = True\n",
    "    if entry['source'] == \"direct-sharing-138\":\n",
    "        moves_data_url = entry['download_url']\n",
    "        has_moves = True\n",
    "if not has_twitter:\n",
    "    print(\"YOU NEED TO HAVE SOME TWITTER DATA IN YOUR ACCOUNT TO USE THIS NOTEBOOK\")\n",
    "    print(\"GO TO http://twarxiv.org TO UPLOAD IT\")\n",
    "\n",
    "\n",
    "\n",
    "# read the twitter data\n",
    "#twitter_data = create_main_dataframe(zip_url=twitter_data_url)\n",
    "\n",
    "# load json file: convert js file to json and then read it\n",
    "twitter_file = 'C:/Users/lyubo/Documents/PYTHON/jupiter_notebook/twitter_data_analysis/account1/data/js/tweet_index.js'\n",
    "\n",
    "\n",
    "\n",
    "with open(twitter_file) as dataFile:\n",
    "    data = dataFile.read()\n",
    "    obj = data[data.find('{') : data.rfind('}')+1]\n",
    "    jsonObj = json.loads(obj)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
